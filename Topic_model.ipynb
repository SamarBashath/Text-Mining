{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Samar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2372036"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Since I am only intersted in headline_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = data[['headline_text']]\n",
    "documents['index'] = documents.index\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters\n",
    "documents['clean_documents'] = documents['headline_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "#remove words have letters less than 3\n",
    "documents['clean_documents'] = documents['clean_documents'].fillna('').apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "#lowercase all characters\n",
    "documents['clean_documents'] = documents['clean_documents'].fillna('').apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# tokenization\n",
    "tokenized_doc = documents['clean_documents'].fillna('').apply(lambda x: x.split())\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(documents)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "documents['clean_documents'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "      <th>clean_documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "      <td>aba decides community broadcasting licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "      <td>act fire witnesses must aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "      <td>calls infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "      <td>air staff aust strike pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "      <td>air strike affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index  \\\n",
       "0  aba decides against community broadcasting lic...      0   \n",
       "1     act fire witnesses must be aware of defamation      1   \n",
       "2     a g calls for infrastructure protection summit      2   \n",
       "3           air nz staff in aust strike for pay rise      3   \n",
       "4      air nz strike to affect australian travellers      4   \n",
       "\n",
       "                              clean_documents  \n",
       "0  aba decides community broadcasting licence  \n",
       "1    act fire witnesses must aware defamation  \n",
       "2      calls infrastructure protection summit  \n",
       "3              air staff aust strike pay rise  \n",
       "4     air strike affect australian travellers  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [aba, decides, community, broadcasting, licence]\n",
       "Name: clean_documents, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_doc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(tokenized_doc)\n",
    "\n",
    "# Create Corpus\n",
    "texts = tokenized_doc\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.052*\"korean\" + 0.041*\"continues\" + 0.033*\"help\" + 0.033*\"calls\" + 0.028*\"drug\" + 0.028*\"toll\" + 0.026*\"summit\" + 0.022*\"subway\" + 0.017*\"community\" + 0.017*\"climb\"\n",
      "Topic: 1 Word: 0.034*\"air\" + 0.032*\"wins\" + 0.023*\"low\" + 0.021*\"home\" + 0.021*\"defamation\" + 0.021*\"river\" + 0.019*\"forces\" + 0.019*\"cuts\" + 0.019*\"aware\" + 0.019*\"must\"\n",
      "Topic: 2 Word: 0.133*\"war\" + 0.050*\"aid\" + 0.036*\"million\" + 0.031*\"call\" + 0.030*\"ethanol\" + 0.026*\"iraq\" + 0.023*\"chief\" + 0.019*\"australia\" + 0.015*\"fuel\" + 0.015*\"become\"\n",
      "Topic: 3 Word: 0.039*\"profit\" + 0.026*\"records\" + 0.026*\"spill\" + 0.023*\"firefighters\" + 0.023*\"policy\" + 0.014*\"leaves\" + 0.012*\"third\" + 0.005*\"net\" + 0.005*\"successive\" + 0.005*\"freedom\"\n",
      "Topic: 4 Word: 0.072*\"injured\" + 0.053*\"council\" + 0.038*\"record\" + 0.026*\"security\" + 0.023*\"club\" + 0.023*\"decision\" + 0.021*\"take\" + 0.015*\"open\" + 0.014*\"smoking\" + 0.010*\"birthday\"\n",
      "Topic: 5 Word: 0.117*\"fire\" + 0.113*\"govt\" + 0.041*\"still\" + 0.028*\"tells\" + 0.022*\"crean\" + 0.018*\"missing\" + 0.017*\"shut\" + 0.013*\"soldiers\" + 0.009*\"anger\" + 0.004*\"alp\"\n",
      "Topic: 6 Word: 0.131*\"police\" + 0.052*\"offer\" + 0.039*\"murder\" + 0.028*\"unlikely\" + 0.020*\"case\" + 0.018*\"consider\" + 0.018*\"sex\" + 0.014*\"confidence\" + 0.012*\"greens\" + 0.004*\"offenders\"\n",
      "Topic: 7 Word: 0.086*\"iraq\" + 0.045*\"house\" + 0.034*\"expected\" + 0.006*\"dargo\" + 0.006*\"white\" + 0.006*\"threat\" + 0.006*\"rebuilding\" + 0.006*\"rise\" + 0.001*\"resolution\" + 0.001*\"shortly\"\n",
      "Topic: 8 Word: 0.088*\"nsw\" + 0.065*\"back\" + 0.057*\"death\" + 0.017*\"mans\" + 0.010*\"hanson\" + 0.005*\"accidental\" + 0.005*\"finds\" + 0.005*\"inquest\" + 0.005*\"came\" + 0.000*\"bangkok\"\n",
      "Topic: 9 Word: 0.067*\"funds\" + 0.044*\"move\" + 0.041*\"ahead\" + 0.029*\"plans\" + 0.021*\"growth\" + 0.016*\"victims\" + 0.010*\"allocated\" + 0.005*\"violence\" + 0.005*\"domestic\" + 0.005*\"bathhouse\"\n",
      "Topic: 10 Word: 0.072*\"tas\" + 0.020*\"break\" + 0.018*\"downs\" + 0.013*\"northern\" + 0.013*\"predicted\" + 0.006*\"tie\" + 0.006*\"thriller\" + 0.006*\"philippoussis\" + 0.006*\"dent\" + 0.006*\"restrictions\"\n",
      "Topic: 11 Word: 0.050*\"backs\" + 0.049*\"meeting\" + 0.046*\"health\" + 0.006*\"storage\" + 0.006*\"tissue\" + 0.006*\"minister\" + 0.006*\"organ\" + 0.006*\"tick\" + 0.006*\"costs\" + 0.006*\"clearance\"\n",
      "Topic: 12 Word: 0.090*\"court\" + 0.046*\"aust\" + 0.041*\"qld\" + 0.032*\"pay\" + 0.023*\"strike\" + 0.021*\"work\" + 0.021*\"hijack\" + 0.015*\"man\" + 0.015*\"central\" + 0.015*\"defeat\"\n",
      "Topic: 13 Word: 0.060*\"water\" + 0.016*\"focus\" + 0.013*\"woes\" + 0.006*\"wollongong\" + 0.006*\"councillor\" + 0.006*\"independent\" + 0.006*\"contest\" + 0.006*\"hill\" + 0.006*\"broken\" + 0.006*\"harrington\"\n",
      "Topic: 14 Word: 0.059*\"plane\" + 0.044*\"four\" + 0.029*\"aussie\" + 0.019*\"memphis\" + 0.015*\"investigation\" + 0.012*\"dismisses\" + 0.012*\"drink\" + 0.012*\"plant\" + 0.012*\"alcohol\" + 0.010*\"reports\"\n",
      "Topic: 15 Word: 0.132*\"man\" + 0.041*\"win\" + 0.027*\"risk\" + 0.022*\"seat\" + 0.020*\"states\" + 0.017*\"leads\" + 0.016*\"chemical\" + 0.016*\"recover\" + 0.016*\"push\" + 0.010*\"united\"\n",
      "Topic: 16 Word: 0.041*\"national\" + 0.030*\"change\" + 0.029*\"opp\" + 0.024*\"england\" + 0.022*\"three\" + 0.020*\"match\" + 0.012*\"wales\" + 0.006*\"insurance\" + 0.006*\"fed\" + 0.006*\"introduce\"\n",
      "Topic: 17 Word: 0.080*\"british\" + 0.049*\"raid\" + 0.024*\"coast\" + 0.021*\"embassy\" + 0.018*\"gold\" + 0.018*\"fined\" + 0.016*\"tent\" + 0.016*\"aboriginal\" + 0.012*\"interest\" + 0.012*\"sought\"\n",
      "Topic: 18 Word: 0.036*\"protesters\" + 0.036*\"crash\" + 0.034*\"plan\" + 0.028*\"hold\" + 0.025*\"group\" + 0.024*\"miss\" + 0.023*\"big\" + 0.022*\"investigations\" + 0.020*\"boost\" + 0.019*\"charged\"\n",
      "Topic: 19 Word: 0.048*\"urged\" + 0.034*\"protect\" + 0.031*\"bushfire\" + 0.022*\"centrelink\" + 0.018*\"moves\" + 0.015*\"heritage\" + 0.006*\"neighbours\" + 0.006*\"continued\" + 0.006*\"iraqs\" + 0.006*\"plead\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(documents['clean_documents'])\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "nsw\n",
      " \n",
      "govt\n",
      " \n",
      "opp\n",
      " \n",
      "hanson\n",
      " \n",
      "war\n",
      " \n",
      "drought\n",
      " \n",
      "iraq\n",
      " \n",
      "Topic 1: \n",
      "war\n",
      " \n",
      "council\n",
      " \n",
      "iraq\n",
      " \n",
      "anti\n",
      " \n",
      "protesters\n",
      " \n",
      "criticism\n",
      " \n",
      "man\n",
      " \n",
      "Topic 2: \n",
      "man\n",
      " \n",
      "court\n",
      " \n",
      "face\n",
      " \n",
      "new\n",
      " \n",
      "council\n",
      " \n",
      "murder\n",
      " \n",
      "injured\n",
      " \n",
      "Topic 3: \n",
      "council\n",
      " \n",
      "chief\n",
      " \n",
      "offer\n",
      " \n",
      "welcomes\n",
      " \n",
      "land\n",
      " \n",
      "decision\n",
      " \n",
      "security\n",
      " \n",
      "Topic 4: \n",
      "funds\n",
      " \n",
      "help\n",
      " \n",
      "korean\n",
      " \n",
      "subway\n",
      " \n",
      "allocated\n",
      " \n",
      "victims\n",
      " \n",
      "miss\n",
      " \n",
      "Topic 5: \n",
      "new\n",
      " \n",
      "high\n",
      " \n",
      "british\n",
      " \n",
      "claim\n",
      " \n",
      "court\n",
      " \n",
      "asylum\n",
      " \n",
      "govt\n",
      " \n",
      "Topic 6: \n",
      "injured\n",
      " \n",
      "crash\n",
      " \n",
      "head\n",
      " \n",
      "highway\n",
      " \n",
      "nightclub\n",
      " \n",
      "new\n",
      " \n",
      "claim\n",
      " \n",
      "Topic 7: \n",
      "korean\n",
      " \n",
      "subway\n",
      " \n",
      "death\n",
      " \n",
      "toll\n",
      " \n",
      "missing\n",
      " \n",
      "continues\n",
      " \n",
      "south\n",
      " \n",
      "Topic 8: \n",
      "iraq\n",
      " \n",
      "plan\n",
      " \n",
      "million\n",
      " \n",
      "pay\n",
      " \n",
      "second\n",
      " \n",
      "british\n",
      " \n",
      "police\n",
      " \n",
      "Topic 9: \n",
      "claim\n",
      " \n",
      "plan\n",
      " \n",
      "raid\n",
      " \n",
      "police\n",
      " \n",
      "water\n",
      " \n",
      "embassy\n",
      " \n",
      "aboriginal\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc )\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(X, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
